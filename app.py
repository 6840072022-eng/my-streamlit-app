# app.py
# VERSIONS: Mock LLM ‚Äî no external API required.
# NOTE: For production with real LLM replace mock functions with API calls.
#
# If you deploy on Streamlit Cloud, add a requirements.txt with:
# streamlit
# requests
# beautifulsoup4
# pandas

import streamlit as st
import requests
import re
import io
import pandas as pd
from collections import Counter
from typing import List, Dict

st.set_page_config(page_title="‡∏™‡∏£‡∏∏‡∏õ‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏° + ‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå + ‡πÅ‡∏ö‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö (Mock)", layout="wide")
st.title("üìò Practicing Reading Skill from a Passage")
st.caption("‡∏¢‡∏Å‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô ‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å BBC, Medium, The Guardian, National Geographic ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô")


# -----------------------
# Helpers: safe extraction
# -----------------------
def fetch_html(url: str, timeout=10) -> str:
    try:
        r = requests.get(url, timeout=timeout, headers={"User-Agent": "Mozilla/5.0"})
        r.raise_for_status()
        return r.text
    except Exception as e:
        return ""

def extract_text_from_html(html: str) -> str:
    # Try BeautifulSoup if available, otherwise simple regex fallback
    try:
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(html, "html.parser")
        # remove scripts/styles
        for s in soup(["script", "style", "noscript"]):
            s.extract()
        paragraphs = soup.find_all("p")
        text = " ".join([p.get_text(separator=" ", strip=True) for p in paragraphs])
        text = re.sub(r"\s+", " ", text).strip()
        return text
    except Exception:
        # fallback: simple regex to take text between tags
        text = re.sub(r"<[^>]+>", " ", html)
        text = re.sub(r"\s+", " ", text).strip()
        return text

def split_into_sentences(text: str) -> List[str]:
    # very simple sentence splitter
    sents = re.split(r'(?<=[.!?])\s+', text.strip())
    sents = [s.strip() for s in sents if len(s.strip()) > 10]
    return sents

# -----------------------
# Mock "LLM" utilities
# -----------------------
STOPWORDS = set([
    "the","and","is","in","to","of","a","for","on","that","with","as","are","was","be",
    "it","by","an","from","at","this","which","or","have","has","but","not","they","their",
    "we","will","can","also","its","such","these","been","more","may"
])

def summarize_mock(text: str, max_sentences: int = 3) -> str:
    sents = split_into_sentences(text)
    if not sents:
        return "(‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏™‡∏£‡∏∏‡∏õ)"
    # Simple extractive summary: pick first N sensible sentences
    return " ".join(sents[:max_sentences])

def extract_keywords_mock(text: str, top_k: int = 8) -> List[str]:
    # crude tokenization and frequency
    words = re.findall(r"\b[a-zA-Z']{3,}\b", text.lower())
    words = [w for w in words if w not in STOPWORDS]
    freq = Counter(words)
    common = [w for w, _ in freq.most_common(top_k)]
    return common

def make_vocab_entries(words: List[str]) -> List[Dict]:
    # create mock meanings and example sentences
    entries = []
    for w in words:
        meaning = f"Mock meaning of '{w}' (a short definition)."
        example = f"Example sentence using {w} in context."
        entries.append({"word": w, "meaning": meaning, "example": example})
    return entries

def generate_questions_mock(text: str, n_questions: int = 5) -> List[Dict]:
    # generate simple comprehension questions (mock)
    sents = split_into_sentences(text)
    questions = []
    # Q1: main idea
    if sents:
        questions.append({"q": "What is the main idea of the article?", "answer_hint": sents[0][:200]})
    # detail questions from later sentences
    for i in range(1, min(len(sents), n_questions)):
        q = f"What is one detail mentioned in sentence {i+1}?"
        questions.append({"q": q, "answer_hint": sents[i][:200]})
    # if not enough, create vocab-in-context questions
    keywords = extract_keywords_mock(text, top_k=5)
    for k in keywords[:max(0, n_questions - len(questions))]:
        questions.append({"q": f"What does '{k}' most likely mean in the article?", "answer_hint": f"Look for usage of '{k}' in the passage."})
    return questions[:n_questions]

# -----------------------
# Session state setup
# -----------------------
if "article_text" not in st.session_state:
    st.session_state.article_text = ""  # store as string
if "summary" not in st.session_state:
    st.session_state.summary = ""
if "vocab_list" not in st.session_state:
    st.session_state.vocab_list = []  # list of dicts: {word, meaning, example}
if "questions" not in st.session_state:
    st.session_state.questions = []  # list of dicts: {q, answer_hint}
if "answers" not in st.session_state:
    st.session_state.answers = []  # user answers

# -----------------------
# UI: input area
# -----------------------
st.sidebar.header("‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤")
st.sidebar.markdown("‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô *mock* ‚Äî ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏à‡∏£‡∏¥‡∏á ‡πÉ‡∏´‡πâ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏° LLM API")

url = st.text_input("‡∏ß‡∏≤‡∏á URL ‡∏Ç‡∏≠‡∏á‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡∏∏‡∏õ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà")

col_fetch, col_clear = st.columns([1, 1])
with col_fetch:
    if st.button("‡∏î‡∏∂‡∏á‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å URL", key="btn_fetch"):
        if not url.strip():
            st.error("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ß‡∏≤‡∏á URL ‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏°")
        else:
            html = fetch_html(url.strip())
            if not html:
                st.error("‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏î‡πâ ‡∏•‡∏≠‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö URL ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏≠‡∏¥‡∏ô‡πÄ‡∏ó‡∏≠‡∏£‡πå‡πÄ‡∏ô‡πá‡∏ï")
            else:
                text = extract_text_from_html(html)
                if not text:
                    st.error("‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö‡∏ô‡∏µ‡πâ")
                else:
                    st.session_state.article_text = text
                    st.success("‡∏î‡∏∂‡∏á‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß")
with col_clear:
    if st.button("‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î", key="btn_clear"):
        st.session_state.article_text = ""
        st.session_state.summary = ""
        st.session_state.vocab_list = []
        st.session_state.questions = []
        st.session_state.answers = []
        st.success("‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡πâ‡∏ß")

st.markdown("---")

# -----------------------
# Show article (preview)
# -----------------------
if st.session_state.article_text:
    st.subheader("üìÑ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏° (‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á 1500 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÅ‡∏£‡∏Å)")
    st.write(st.session_state.article_text[:1500] + ("..." if len(st.session_state.article_text) > 1500 else ""))

    col1, col2, col3 = st.columns([1,1,1])
    with col1:
        if st.button("‡∏™‡∏£‡∏∏‡∏õ‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏° (‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏ö‡∏ö‡∏¢‡πà‡∏≠)", key="btn_summarize"):
            st.session_state.summary = summarize_mock(st.session_state.article_text)
            st.success("‡∏™‡∏£‡∏∏‡∏õ‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß")
    with col2:
        if st.button("‡∏î‡∏∂‡∏á‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏à‡∏≤‡∏Å‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°", key="btn_vocab"):
            keywords = extract_keywords_mock(st.session_state.article_text, top_k=12)
            st.session_state.vocab_list = make_vocab_entries(keywords)
            st.success("‡∏î‡∏∂‡∏á‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß")
    with col3:
        if st.button("‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ö‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏à‡∏±‡∏ö‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏° (‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©)", key="btn_questions"):
            st.session_state.questions = generate_questions_mock(st.session_state.article_text, n_questions=5)
            # reset answers
            st.session_state.answers = [""] * len(st.session_state.questions)
            st.success("‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß")

# -----------------------
# Show summary & vocab table
# -----------------------
if st.session_state.summary:
    st.subheader("üìù ‡∏™‡∏£‡∏∏‡∏õ‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏° (‡πÅ‡∏ö‡∏ö‡∏¢‡πà‡∏≠)")
    st.write(st.session_state.summary)

if st.session_state.vocab_list:
    st.subheader("üìö ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç")
    # build dataframe temporarily from session_state list (do not store DF in session)
    df_vocab = pd.DataFrame(st.session_state.vocab_list)
    # show table with Thai headers
    df_show = df_vocab.rename(columns={"word":"‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå","meaning":"‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢","example":"‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á"})
    st.dataframe(df_show)
    # download CSV
    csv_buf = df_vocab.to_csv(index=False)
    st.download_button("‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå (CSV)", csv_buf, file_name="vocabulary.csv")

# -----------------------
# Quiz / Test UI
# -----------------------
if st.session_state.questions:
    st.subheader("üß™ ‡πÅ‡∏ö‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏à‡∏±‡∏ö‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏° (‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©)")
    st.write("‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏© ‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡∏î '‡∏ï‡∏£‡∏ß‡∏à‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡πà‡∏≤‡∏ß ‡πÜ (mock grading)")
    form = st.form(key="quiz_form")
    user_answers = []
    for i, q in enumerate(st.session_state.questions):
        form.write(f"**Q{i+1}. {q['q']}**")
        # prefill with existing if any
        existing = st.session_state.answers[i] if i < len(st.session_state.answers) else ""
        ans = form.text_area(f"Your answer for Q{i+1}", value=existing, key=f"ans_{i}")
        user_answers.append(ans)
        form.markdown("---")
    submitted = form.form_submit_button("‡∏™‡πà‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö")
    if submitted:
        # save answers
        st.session_state.answers = user_answers
        # grading (very simple mock): check overlap of important words from hint
        scores = []
        for ans, q in zip(user_answers, st.session_state.questions):
            hint_words = set(re.findall(r"\b[a-zA-Z']{4,}\b", q.get("answer_hint","").lower()))
            ans_words = set(re.findall(r"\b[a-zA-Z']{4,}\b", ans.lower()))
            if not hint_words:
                score = 1 if len(ans.strip())>10 else 0
            else:
                overlap = len(hint_words & ans_words)
                score = 1 if overlap >= max(1, len(hint_words)//4) else 0
            scores.append(score)
        total = sum(scores)
        st.success(f"‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏î‡πâ {total} ‡∏à‡∏≤‡∏Å {len(scores)} ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (‡∏£‡∏∞‡∏ö‡∏ö‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡πà‡∏≤‡∏ß ‡πÜ)")
        # show per-question feedback
        for i, s in enumerate(scores):
            st.write(f"Q{i+1}: {'‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (‡∏ï‡∏≤‡∏°‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö)' if s>0 else '‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô - ‡∏•‡∏≠‡∏á‡∏ï‡∏≠‡∏ö‡πÉ‡∏´‡∏°‡πà'}")
            st.caption(f"‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á hint: {st.session_state.questions[i]['answer_hint'][:200]}")

st.markdown("---")
st.caption("‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô Mock ‚Äî ‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏™‡∏£‡∏∏‡∏õ/‡∏î‡∏∂‡∏á‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå/‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏∂‡πâ‡∏ô ‡πÉ‡∏´‡πâ‡∏ö‡∏≠‡∏Å‡∏â‡∏±‡∏ô‡∏à‡∏∞‡∏ú‡∏ô‡∏ß‡∏Å LLM API ‡πÉ‡∏´‡πâ (OpenAI / Google Gemini ‡∏Ø‡∏•‡∏Ø).")

